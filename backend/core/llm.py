import httpx
import logging
import json
from backend.core.config import settings

logger = logging.getLogger(__name__)

class LLMClient:
    def __init__(self):
        # vLLM is running on a specific port (mapped to 8001 in docker-compose)
        # However, inside the docker network, it is accessible via the service name 'vllm' and port 8000
        self.base_url = "http://vllm:8000/v1" 
        self.model = "Qwen/Qwen2.5-7B-Instruct-AWQ"
        self.headers = {
            "Content-Type": "application/json",
            # "Authorization": f"Bearer {settings.HUGGING_FACE_HUB_TOKEN}" # Not needed for local vllm usually unless gated
        }
        
    async def process_message(self, user_message: str, context_data: str = None) -> str:
        """
        Sends a message to the local LLM and returns the response.
        """
        system_prompt = """Voc√™ √© o Cortex Brasil, um assistente financeiro pessoal, s√°bio e proativo.
Seu objetivo √© extrair informa√ß√µes financeiras de mensagens informais e realizar a contabilidade correta (Double-Entry).

Sempre responda em formato JSON estrito, sem markdown, com a seguinte estrutura:
{
    "action": "log_transaction" | "chat",
    "data": {
        "amount": float | null,
        "type": "EXPENSE" | "INCOME" | "TRANSFER",
        "category": string | null,
        "description": string | null,
        "account_name": string | null ("Nubank", "Ita√∫", "Carteira", "Cofre"),
        "destination_account_name": string | null (Apenas para TRANSFER),
        "date": string (ISO 8601) | null,
        "installments": integer | null
    },
    "reply_text": "Sua resposta curta e amig√°vel para o usu√°rio aqui."
}

## REGRAS DE CONTABILIDADE
1. **GASTOS (EXPENSE):** "Gastei 50 no almo√ßo", "Comprei um livro".
   - `account_name`: De onde saiu o dinheiro? Se n√£o falado, assuma "Carteira".
2. **ENTRADAS (INCOME):** "Recebi 5000 de sal√°rio", "Caiu um pix de 50".
   - `category`: "Sal√°rio", "Renda Extra", "Reembolso".
3. **TRANSFER√äNCIAS (TRANSFER):** "Paguei o cart√£o Nubank com o Ita√∫", "Mandei 500 pra poupan√ßa".
   - `account_name`: Origem (De onde saiu).
   - `destination_account_name`: Destino (Para onde foi).

## CONTEXTO FINANCEIRO (Mem√≥ria Recente e Contas)
Use os dados abaixo para responder perguntas sobre hist√≥rico, saldos ou h√°bitos.
--------------------------------------------------
{context_data}
--------------------------------------------------

Exemplos:
Usuario: "Gastei 50 no mcdonalds no d√©bito do itau"
Resposta: {
    "action": "log_transaction",
    "data": {"amount": 50.0, "type": "EXPENSE", "category": "Alimenta√ß√£o", "description": "McDonalds", "account_name": "Ita√∫", "installments": null},
    "reply_text": "Registrado: R$ 50,00 no Ita√∫ (Alimenta√ß√£o). üçî"
}

Usuario: "Recebi 5000 da empresa"
Resposta: {
    "action": "log_transaction",
    "data": {"amount": 5000.0, "type": "INCOME", "category": "Sal√°rio", "description": "Sal√°rio Empresa", "account_name": "Ita√∫", "installments": null},
    "reply_text": "Boa! R$ 5.000,00 de entrada registrados. üí∏"
}
"""
        
        # Inject context or empty string
        formatted_prompt = system_prompt.replace("{context_data}", context_data if context_data else "Nenhuma transa√ß√£o recente encontrada.")

        messages = [{"role": "system", "content": formatted_prompt}]
        messages.append({"role": "user", "content": user_message})

        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": 0.3,
            "max_tokens": 1000 # Increased for context answers
        }

        async with httpx.AsyncClient(timeout=120.0) as client:
            try:
                logger.info(f"Sending request to LLM: {self.model}")
                response = await client.post(f"{self.base_url}/chat/completions", headers=self.headers, json=payload)
                
                if response.status_code != 200:
                    logger.error(f"LLM Error {response.status_code}: {response.text}")
                    return json.dumps({
                        "action": "chat",
                        "reply_text": "Desculpe, estou com dificuldades t√©cnicas no momento."
                    })
                    
                result = response.json()
                content = result['choices'][0]['message']['content']
                return content
            except httpx.ConnectError:
                logger.error("Could not connect to vLLM service.")
                return json.dumps({
                    "reply_text": "üß† O Cortex est√° acordando. Tente novamente em alguns segundos."
                })
            except httpx.ReadTimeout:
                logger.error("LLM Request Timed Out")
                return json.dumps({
                    "action": "chat", 
                    "reply_text": "üß† O processamento est√° demorando mais que o esperado. Tente uma frase mais curta?"
                })
            except Exception as e:
                logger.error(f"Error calling LLM: {str(e)}")
                return json.dumps({
                    "action": "chat", 
                    "reply_text": "Ops, tive um pensamento confuso. Tente novamente."
                })
